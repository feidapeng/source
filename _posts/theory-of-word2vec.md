title: 深入浅出word2vec
date: 2015-10-01 12:30:16
categories: research
tags: word2vec
---
# 前言 #
　　最近一直在研究情感计算相关，那么词向量word embedding的基础知识必不可少。而目前最火的工具就是google在2013年推出的word2vec，上个学期虽然用过它处理语料，但是对于其原理仅仅只是一知半解。那么问题来了，既然要做这方面的研究，当然是要知其然也要知其所以然咯。  
　　经过几天的调研、研究、啃源码，算是对word2vec有了一个全新的认识，值此国庆佳节之际写一下个人对它的理解，亦是一种学习的总结。

----------
# 背景 #
　　NLP要将文本转化为计算机能理解的范畴，首要任务肯定是把文字数学化、符号化。将word映射到一个新的空间中，并以多维的连续实数向量进行表示叫做“*word representation*”或者“*word embedding*”，中文就叫做词向量。  

　　最直观也是最简单的表示方法就是**one-hot representation**，这是一种稀疏表示法，将每个word表示成一个很长的向量，向量的维数表示为word的数量，只有一个维度是1，剩下的都是0。  
　　举个栗子：  
　　　　篮球{0，0，0，0，1，0，0，0，0，…………，0}  
　　　　篮板{0，0，0，0，0，0，0，1，0，…………，0}  
　　这种表示方法像是给每个word分配了一个ID，如果编程的话用hash便可以解决。但是当语料很大，向量维度就会一直大下去，最重要的是这种方法会出现“词汇鸿沟”现象：任意两个词之间都是孤立的，不能通过词向量看出它们之间的任何关系，就连“篮球”和“篮板”也呵呵了。

  
　　当然获得词向量的方法还有很多，例如LSA、LDA，而word2vec利用了词的上下文，语义信息更加丰富，并且其训练过程十分高效。

　　word2vec中词向量表示法被称为**distributed representation**，不同于one-hot，这种向量一般是100维、200维居多，大概长成这个样子{-0.452，0.896，0.330，-0.278，…………}，有了这样的向量，想找到word之间的关系so easy。

----------
# 预备知识 #

掌握了这些，妈妈再也不用担心我的学习  

- sigmoid函数：推一遍导函数公式咯
- Logistic Regression：损失函数、似然函数相关，代码跑一遍感觉棒棒哒
- Bayes公式：你懂的
- Huffman编码：按词频编码，需注意word2vec中约定词频大的结点为左孩子，词频小的为右孩子，且左孩子编码为1，右孩子编码为0。即： **左大右小、左1右0**


----------
# 模型分析 #
word2vec中用到了两个重要的模型，分别是CBOW（continuous bag-of-words model）和skip-gram（continuous skip-gram model）。示意图如下：
![](http://i13.tietuku.com/4f9f36ca8715ee53.png)
由图可见，两个模型都包含三层：输入层、映射层和输出层，说是deep learning原来也不deep。  

前者是在已知当前词w(t)的上下文的前提下预测w(t)，后者恰恰相反。


我们来看基于Hierarchical Softmax的cbow模型，它的结构是这样的：
![](http://i13.tietuku.com/9eff43ce6e55b234.png)


- 输入层读入窗口内的词，将它们的向量（K维，初始随机）加和在一起，形成隐藏层K个节点。  
- 输出层是一个巨大的Huffman树，叶节点代表语料里所有的词（语料含有V个独立的词，则二叉树有|V|个叶节点）。对于叶节点上的每一个词，都有一个全局唯一的huffman编码，例如“001101”。
- 隐层的每一个节点都会跟二叉树的内节点有连边，于是对于二叉树的每一个内节点都会有K条连边，每条边上也会有权值。  

在训练阶段，当预测某个词Wt时，前期Huffman树已构建完成，所以这时Wt肯定存在于Huffman树中，并且有一个唯一的编码，例如“001101”，那么接下来我们就从根节点一个个去遍历，我们的目标是使得预测词的二进制编码概率最大。形象地说，我们希望在根节点，词向量和与根节点相连经过logistic计算得到的概率尽量接近0（因为预测目标是bit=0）；在第二层，希望其bit仍然是0，即概率尽量接近0……这么一直下去，我们把一路上计算得到的概率相乘，即得到目标词Wt在当前网络下的概率(P(Wt))，那么对于当前这个sample的残差就是1-P(Wt)。于是就可以使用随机梯度上升优化各种权值了。


----------
写到这里，感觉还是没有想象中的那么清楚，想到什么再补充吧。
### 参考资料 ###
[word2vec中的数学原理详解](http://blog.csdn.net/itplus/article/details/37969519)  
[word2vec源码解析之word2vec.c](http://blog.csdn.net/lingerlanlan/article/details/38232755)  
[深度学习word2vec笔记之基础篇](http://blog.csdn.net/mytestmy/article/details/26961315)  
[文本深度表示模型Word2Vec](http://wei-li.cnblogs.com/p/word2vec.html)